{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d34c32",
   "metadata": {},
   "source": [
    "# Problem 2.5\n",
    "\n",
    "Algorithm for batch size finding (which should exponentially increase)\n",
    "\n",
    "1. Set small and large batch size bounds\n",
    "2. Start training network\n",
    "3. Decrease batch size exponentially after each batch update\n",
    "4. Record loss and batch size at end of each batch\n",
    "5. Train for 3-5 epochs\n",
    "6. Plot loss and batch size \n",
    "7. Examine plot and identify optimal Batch Size\n",
    "8. Update batch size\n",
    "9. Train network on full set of data\n",
    "\n",
    "Modified to reflect how we found bmin and bmax\n",
    "1. Set small and large batch size bounds, Set number of iterations\n",
    "2. Train network with batch size and epoch that lead to same number of iterations\n",
    "4. Record loss and batch size at end \n",
    "6. Plot loss and batch size \n",
    "7. Examine plot and identify optimal Batch Size min and batch size max\n",
    "8. Update batch size\n",
    "9. Train network on full set of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b747192",
   "metadata": {},
   "source": [
    "# Problem 2.6\n",
    "\n",
    "Learning rate decrease is equivalent to batch size increase. So the analogous trajectory for exponential decay of learning rate would be to exponentially increase the batch size. \n",
    "\n",
    "## cyclical batch size policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99dc83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
